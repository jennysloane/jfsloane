---
draft: false
abstract: In three experiments, we sought to understand when and why people use an algorithm decision aid. Distinct from recent approaches, we explicitly enumerate the algorithm’s accuracy while also providing summary feedback and training that allowed participants to assess their own skills. Our results highlight that such direct performance comparisons between the algorithm and the individual encourages a strategy of selective reliance on the decision aid; individuals ignored the algorithm when the task was easier and relied on the algorithm when the task was harder. Our systematic investigation of summary feedback, training experience, and strategy hint manipulations shows that further opportunities to learn about the algorithm encourage not only increased reliance on the algorithm but also engagement in experimentation and verification of its recommendations. Together, our findings emphasize the decision-maker’s capacity to learn about the algorithm providing insights for how we can improve the use of decision aids.

authors:
- Garston Liang
- Jennifer Sloane
- Chris Donkin
- Ben Newell


date: "2022-03-25T00:00:00Z"
doi: ""
featured: true
image:
  caption: ''
  focal_point: ""
  preview_only: false
projects: []
publication: ''
publication_short: ""
 # publication_types:
 # - "2"
publishDate: "2022-09T00:00:00Z"
slides: 
summary: 
tags:
- Source Themes
title: "Adapting to the algorithm: how accuracy comparisons promote the use of a decision aid."
url_code: ""
url_dataset: ""
url_pdf: "https://link.springer.com/article/10.1186/s41235-022-00364-y"
url_poster: ""
url_project: ""
url_slides: ""
url_source: ""
url_video: ""
---

